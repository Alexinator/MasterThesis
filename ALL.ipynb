{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import shutil\n",
    "from multiprocessing import Pool, cpu_count, Manager\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wfdb\n",
    "import neurokit2 as nk\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import lfilter, iirnotch, detrend, butter, filtfilt, savgol_filter, freqz\n",
    "from scipy import signal, stats\n",
    "from statsmodels.robust import mad\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, LSTM, Masking, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D, \n",
    "                                     Input, add, Activation, BatchNormalization, GlobalAveragePooling2D, Dropout)\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Define directories\n",
    "nsrdb_dir = \"C:/Users/alexc/Desktop/MasterThesis/data/nsrdb\"  # replace with your actual directory\n",
    "afdb_dir = \"C:/Users/alexc/Desktop/MasterThesis/data/afdb\"  # replace with your actual directory\n",
    "mitdb_dir = \"C:/Users/alexc/Desktop/MasterThesis/data/mitdb\"  # replace with your actual directory\n",
    "\n",
    "results_dir = \"C:/Users/alexc/Desktop/MasterThesis/results/\"  # replace with your actual directory\n",
    "preprocessing_dir = 'C:/Users/alexc/Desktop/MasterThesis/preprocessing/'  # replace with your actual directory\n",
    "\n",
    "sec_segments_dir = preprocessing_dir  # replace with your actual directory\n",
    "beats_segments_dir = preprocessing_dir # replace with your actual directory\n",
    "RR_segments_dir = preprocessing_dir # replace with your actual directory\n",
    "pwave_segments_dir = preprocessing_dir  # replace with your actual directory\n",
    "\n",
    "sample_rate = 360 # Sampling rate\n",
    "seconds = 10 # Number of seconds to be extracted from each segment\n",
    "RRs = 301 # Number of RRs to be extracted from each segment\n",
    "n_beats = 100 # Number of beats to be extracted from each segment\n",
    "overlapping = 50 # Overlapping added to each segment\n",
    "lowcut = 0.5  # Low cut-off frequency (Hz)\n",
    "highcut = 50.0  # High cut-off frequency (Hz)\n",
    "min_mV = 0 # Minimum amplitude (mV)\n",
    "max_mV = 1 # Maximum amplitude (mV)\n",
    "n_epochs = 10 # Number of epochs\n",
    "\n",
    "#####################\n",
    "### LOOP ELEMENTS ###\n",
    "#####################  \n",
    "analyse_list = ['seconds','beats','RR','pwave']\n",
    "loop_list = ['manual','neurokit2','raw']\n",
    "quality_list = ['excellent']\n",
    "limited_time_list = [60, 300, 600] # minutes \n",
    "\n",
    "# Set up logging\n",
    "# Try catch to ignore the flushing error when the file do not exists\n",
    "try:\n",
    "    logging.basicConfig(filename='C:/Users/alexc/Desktop/MasterThesis/logs.log', level=logging.INFO, flushing=True)\n",
    "except:\n",
    "    logging.basicConfig(filename='C:/Users/alexc/Desktop/MasterThesis/logs.log', level=logging.INFO, flushing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING AND ELSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and plot the dropped segments percentages per database\n",
    "def plot_segment_percentages(results_subdir, ignored_counts, total_counts):\n",
    "    databases = list(total_counts.keys())\n",
    "    ignored_percentages = [ignored_counts[db] / total_counts[db] * 100 for db in databases]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(databases, ignored_percentages)\n",
    "    ax.set_xlabel('Database')\n",
    "    ax.set_ylabel('Percentage of Ignored Segments (%)')\n",
    "    ax.set_title('Percentage of Ignored Segments per Database')\n",
    "    fig.savefig(os.path.join(results_subdir, 'ignored_segments_plot.png'))\n",
    "    plt.close(fig)\n",
    "    df = pd.DataFrame({\n",
    "        'Database': databases,\n",
    "        'Ignored Counts': [ignored_counts[db] for db in databases],\n",
    "        'Total Counts': [total_counts[db] for db in databases],\n",
    "        'Ignored Percentages': ignored_percentages\n",
    "    })\n",
    "    df.to_csv(os.path.join(results_subdir, 'ignored_segments.csv'), index=False)\n",
    "\n",
    "# Plot the padding percentages per database, used by Beats methodology\n",
    "def plot_padding_percentages(padding_percentages, results_subdir):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Padding Percentages')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_xticks(range(len(padding_percentages)))\n",
    "    ax.set_xticklabels(padding_percentages.keys())\n",
    "    ax.bar(padding_percentages.keys(), padding_percentages.values())\n",
    "    save_plot(fig, results_subdir, \"padding_percentages\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues, results_subdir=None, fileName=None):\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(os.path.join(results_subdir, f'{fileName}_confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Save a plot for a given figure, path and filename\n",
    "def save_plot(fig, path, filename):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    fig.savefig(os.path.join(path, f\"{filename}.png\"), dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Save the metrics and confusion matrix to a CSV file\n",
    "# If the file already exists, append to it\n",
    "def save_metrics_to_csv(metrics, cm, filename, model_info):\n",
    "    metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "    cm_df = pd.DataFrame(cm.flatten(), index=['TN', 'FP', 'FN', 'TP']).T\n",
    "    df = pd.concat([metrics_df, cm_df], axis=1)\n",
    "    df['Model'] = model_info\n",
    "    if os.path.isfile(filename):\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(filename, mode='w', header=True, index=False)\n",
    "\n",
    "# Collect all results from the results directory and save them to a single CSV file\n",
    "def collect_results(results_dir):\n",
    "    df_list = []\n",
    "    for csv_file in glob.glob(results_dir + '/*/*.csv'):\n",
    "        folder_name = os.path.dirname(csv_file).replace(\"\\\\\", \"/\")\n",
    "        folder_name_parts = folder_name.split('/')[-1].split('_')\n",
    "        analyse_type = folder_name_parts[0].replace(\"results\", \"\")\n",
    "        loop_type = folder_name_parts[1]\n",
    "        quality_type = '_'.join(folder_name_parts[2:])\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['Analyse Type'] = analyse_type\n",
    "        df['Loop Type'] = loop_type\n",
    "        df['Quality Type'] = quality_type\n",
    "        df_list.append(df)\n",
    "\n",
    "    complete_results = pd.concat(df_list, ignore_index=True)\n",
    "    column_order = ['Analyse Type', 'Loop Type', 'Quality Type', 'Model', 'accuracy', 'specificity', 'f1_score', 'TN', 'FP', 'FN', 'TP']\n",
    "    complete_results = complete_results[column_order]\n",
    "    complete_results.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "    complete_results.to_csv(os.path.join(results_dir, 'complete_results.csv'), index=False)\n",
    "\n",
    "# Plot the results from the complete results CSV file\n",
    "# We plot the top 10 configurations by accuracy\n",
    "def plot_results(df, results_dir):\n",
    "    df[\"Configuration\"] = df[\"Analyse Type\"] + \"_\" + df[\"Loop Type\"] + \"_\" + df[\"Quality Type\"] + \"_\" + df[\"Model\"]\n",
    "    top_configurations = df.nlargest(10, \"accuracy\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"Configuration\", y=\"accuracy\", data=top_configurations)\n",
    "    plt.xticks(rotation=90) \n",
    "    plt.title(\"Top 10 Configurations by Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, \"results_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# First analyse of the data made after preprocessing\n",
    "def data_analysis(directory, output_directory):\n",
    "    subfolders = [f.path for f in os.scandir(directory) if f.is_dir()]\n",
    "    def count_segments_in_folder(folder):\n",
    "        return len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "    segment_counts = {os.path.basename(folder): count_segments_in_folder(folder) for folder in subfolders}\n",
    "    df = pd.DataFrame.from_dict(segment_counts, orient='index', columns=['segment_count'])\n",
    "    df.index.name = 'database'\n",
    "    df.to_csv(os.path.join(output_directory, 'segment_counts.csv'), index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove subfolders based on the directory path\n",
    "def remove_subfolders(directory):\n",
    "    if os.path.exists(directory):\n",
    "        for item in os.scandir(directory):\n",
    "            if item.is_dir():\n",
    "                shutil.rmtree(item.path)\n",
    "\n",
    "# Load data and labels from the files in the directory\n",
    "def load_data_and_labels(files, directory):\n",
    "    data = [np.loadtxt(os.path.join(directory, file), delimiter=',') for file in files]\n",
    "    labels = [int(file.split('_')[-1][0]) for file in files]  # Extract class label from filename\n",
    "    return data, labels\n",
    "\n",
    "# Rescale the data to the new min and max\n",
    "def rescale_data(data, new_min, new_max):\n",
    "    current_min = np.min(data)\n",
    "    current_max = np.max(data)\n",
    "    rescaled_data = (data - current_min) * (new_max - new_min) / (current_max - current_min) + new_min\n",
    "    return rescaled_data\n",
    "\n",
    "# Rescale the 2D data to the new min and max, used by Peaks\n",
    "def rescale_data_2D(data, new_min, new_max):\n",
    "    current_min = np.min(data, axis=0)\n",
    "    current_max = np.max(data, axis=0)\n",
    "    rescaled_data = (data - current_min) * (new_max - new_min) / (current_max - current_min) + new_min\n",
    "    return rescaled_data\n",
    "\n",
    "# Function to apply a bandpass filter to the data\n",
    "# https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "def apply_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Function to apply a lowpass filter to the data\n",
    "# https://notebook.community/CSchoel/learn-wavelets/wavelet-denoising\n",
    "# Sigma = constant scale factor of normal distribution * MAD\n",
    "def wavelet_denoising(data, wavelet='db5', level=1):\n",
    "    coeff = pywt.wavedec(data, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * mad(coeff[-level])\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(data)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='soft') for i in coeff[1:])\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')\n",
    "\n",
    "# Calculate the padding percentage added to the data, used by Beats\n",
    "def calculate_padding_percentage(data, pad_value=-1):\n",
    "    padding_percentages = []\n",
    "    for array in data:\n",
    "        num_total_values = np.prod(array.shape)\n",
    "        num_padding_values = np.count_nonzero(array == pad_value)\n",
    "        padding_percentage = (num_padding_values / num_total_values) * 100\n",
    "        padding_percentages.append(padding_percentage)\n",
    "    return np.mean(padding_percentages)\n",
    "\n",
    "# Evaluate the model, compute the metrics and save the results to a CSV file\n",
    "def evaluate_model(model, model_name, train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir):\n",
    "    model.fit(train_data, train_labels, epochs=n_epochs, validation_data=(val_data, val_labels))\n",
    "    # Evaluate the model on the test data and retrieve the metrics\n",
    "    test_predictions = (model.predict(test_data) > 0.5).astype(\"int32\")\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    f1 = f1_score(test_labels, test_predictions)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "    save_metrics_to_csv(metrics, cm, os.path.join(results_subdir, 'performance_metrics.csv'), model_name)\n",
    "    plot_confusion_matrix(cm, ['NS', 'AF'], results_subdir=results_subdir, fileName=model_name)\n",
    "    # Plot the ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(val_labels, model.predict(val_data))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_df = pd.DataFrame({\n",
    "        'FPR': fpr,\n",
    "        'TPR': tpr,\n",
    "        'AUC': [roc_auc]*len(fpr) \n",
    "    })\n",
    "    file_name = os.path.join(results_subdir, f'roc_auc_{model_name}.csv')\n",
    "    roc_df.to_csv(file_name, index=False)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(results_subdir, f'{model_name}_roc_curve.png')) \n",
    "    plt.close()\n",
    "\n",
    "# Evaluate the model for RandomForest, compute the metrics and save the results to a CSV file\n",
    "def evaluate_model_RF(model, model_name, train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir):\n",
    "    # Flatten the data because RandomForestClassifier cannot handle 3D data\n",
    "    train_data_flattened = train_data.reshape(train_data.shape[0], -1)\n",
    "    val_data_flattened = val_data.reshape(val_data.shape[0], -1)\n",
    "    test_data_flattened = test_data.reshape(test_data.shape[0], -1)\n",
    "    \n",
    "    model.fit(train_data_flattened, train_labels)\n",
    "\n",
    "    # Evaluate the model on the test data and retrieve the metrics\n",
    "    test_predictions = model.predict(test_data_flattened)\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    f1 = f1_score(test_labels, test_predictions)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "    save_metrics_to_csv(metrics, cm, os.path.join(results_subdir, 'performance_metrics.csv'), model_name)\n",
    "    plot_confusion_matrix(cm, ['NS', 'AF'], results_subdir=results_subdir, fileName=model_name)\n",
    "\n",
    "    # Plot ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(val_labels, model.predict_proba(val_data_flattened)[:, 1]) # Probabilities are needed for ROC curve\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_df = pd.DataFrame({\n",
    "        'FPR': fpr,\n",
    "        'TPR': tpr,\n",
    "        'AUC': [roc_auc]*len(fpr)\n",
    "    })\n",
    "    file_name = os.path.join(results_subdir, f'roc_auc_{model_name}.csv')\n",
    "    roc_df.to_csv(file_name, index=False)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(results_subdir, f'{model_name}_roc_curve.png'))  \n",
    "    plt.close()  \n",
    "\n",
    "# Define the ResNet block for 1D data as described in the paper\n",
    "# https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)31721-0/fulltext\n",
    "def res_block(x, filters, kernel_size=3, stride=1, conv_shortcut=True, pool_size=2):\n",
    "    shortcut = x\n",
    "    if conv_shortcut:\n",
    "        shortcut = layers.Conv1D(filters, 1, strides=stride)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([shortcut, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size)(x)\n",
    "    return x\n",
    "\n",
    "# Create and return the ResNet1D model as described in the paper\n",
    "# https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)31721-0/fulltext\n",
    "def ResNet1D(length, features):\n",
    "    inputs = Input(shape=(length,features))\n",
    "    x = layers.Masking(mask_value=-1.0)(inputs)\n",
    "    for _ in range(10):  # 10 residual blocks\n",
    "        x = res_block(x, 64, pool_size=1) \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ECG files into train, val, and test sets with a 70/10/20 split.\n",
    "def split_files(files, source):\n",
    "    num_total_files = len(files)\n",
    "    train_end_idx = int(0.7 * num_total_files)\n",
    "    val_end_idx = int(0.8 * num_total_files)\n",
    "    \n",
    "    train_files = files[:train_end_idx]\n",
    "    val_files = files[train_end_idx:val_end_idx]\n",
    "    test_files = files[val_end_idx:]\n",
    "    \n",
    "    train_df = pd.DataFrame({'fileName': train_files, 'source': [source]*len(train_files), 'set': 'train'})\n",
    "    val_df = pd.DataFrame({'fileName': val_files, 'source': [source]*len(val_files), 'set': 'val'})\n",
    "    test_df = pd.DataFrame({'fileName': test_files, 'source': [source]*len(test_files), 'set': 'test'})\n",
    "        \n",
    "    return pd.concat([train_df, val_df, test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "def allocate_ecg_to_datasets(nsrdb_dir, afdb_dir, mitdb_dir):\n",
    "    datasets = {\n",
    "        \"nsrdb\": nsrdb_dir,\n",
    "        \"afdb\": afdb_dir,\n",
    "        \"mitdb\": mitdb_dir\n",
    "    }\n",
    "        \n",
    "    all_dfs = []\n",
    "    for source, directory in datasets.items():\n",
    "        files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".dat\")]\n",
    "        np.random.shuffle(files)\n",
    "        df = split_files(files, source)\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    result_df = pd.concat(all_dfs, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "ecg_df = allocate_ecg_to_datasets(nsrdb_dir, afdb_dir, mitdb_dir)\n",
    "ecg_df.to_csv(\"C:/Users/alexc/Desktop/MasterThesis/splits.csv\", index=False)\n",
    "print(ecg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECONDS\n",
    "\n",
    "SECONDS PREPROCESSING\n",
    "\n",
    "Based on the article by Attia et al.\n",
    "\n",
    "https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)31721-0/fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ecg_data_seconds(directory, segments_directory, subfolder, is_nsrdb=False, is_afdb=False, is_mitdb=False, patient_id=None, seconds=10, limited_time=None, loop_type='manual', quality_type='excellent'):\n",
    "    total_segments = 0\n",
    "    ignored_segments = 0\n",
    "\n",
    "    if patient_id is not None:\n",
    "        files = [file for file in os.listdir(directory) if file.startswith(patient_id)]\n",
    "    else:\n",
    "        files = os.listdir(directory)\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith(\".dat\"):\n",
    "            logging.info(f\"Processing file: {file}\")\n",
    "\n",
    "            # Determine where to save the file based on the dataframe\n",
    "            current_set = ecg_df.loc[ecg_df['fileName'] == os.path.join(directory, file), 'set'].values[0]\n",
    "            save_dir = os.path.join(segments_directory, current_set)\n",
    "\n",
    "            record = wfdb.rdrecord(os.path.join(directory, file[:-4]))\n",
    "\n",
    "            # Identify the index of the desired lead\n",
    "            if is_afdb :\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG2 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_nsrdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG1 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_mitdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('MLII')\n",
    "                except ValueError:\n",
    "                    print(f\"No MLII lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "\n",
    "            ecg_signals = record.p_signal[:, lead_index]\n",
    "            window_length = record.fs * seconds  # seconds parameter window\n",
    "\n",
    "            cleaned_ecg = ecg_signals\n",
    "            # If we work with limited time\n",
    "            if limited_time != None:\n",
    "                cleaned_ecg = ecg_signals[:record.fs * 60 * limited_time]\n",
    "                \n",
    "            try:\n",
    "                if is_afdb or is_mitdb:\n",
    "                    logging.info(f\"Processing AFDB or MITDB data for file: {file}\")\n",
    "                    annotation_extension = 'qrs' if is_afdb else 'atr'\n",
    "                    qrs_annotations = wfdb.rdann(os.path.join(directory, file[:-4]), annotation_extension)\n",
    "                    i = 0\n",
    "                    while i < len(qrs_annotations.sample) - 1 and qrs_annotations.sample[i] < len(cleaned_ecg):\n",
    "                        if qrs_annotations.symbol[i] == 'N':\n",
    "                            start = qrs_annotations.sample[i]\n",
    "                            end = min(len(cleaned_ecg), start + window_length)  # 10 seconds after the QRS complex\n",
    "                            window_annotation_indices = [i for i in range(len(qrs_annotations.sample)) if start <= qrs_annotations.sample[i] < end]\n",
    "                            next_symbols = [qrs_annotations.symbol[i] for i in window_annotation_indices]\n",
    "                            if all(symbol == 'N' for symbol in next_symbols):\n",
    "                                # Extract the window\n",
    "                                window = cleaned_ecg[start:end]\n",
    "                                if True: #not np.any(window == 0): not used because always true\n",
    "                                    # Check if the window is exactly the window_length\n",
    "                                    if len(window) == window_length:\n",
    "                                        #################\n",
    "                                        ### Loop type ###\n",
    "                                        #################\n",
    "                                        window = nk.ecg_invert(window, sampling_rate=record.fs)[0]\n",
    "                                        window = nk.signal_resample(window, method=\"FFT\", sampling_rate=record.fs, desired_sampling_rate=sample_rate)\n",
    "                                        if loop_type == 'manual':\n",
    "                                            window = apply_bandpass_filter(window, lowcut, highcut, sample_rate, order=5)\n",
    "                                            window = wavelet_denoising(window)\n",
    "                                        elif loop_type == 'neurokit2':\n",
    "                                            window = nk.ecg_clean(window, sampling_rate=sample_rate, method='neurokit2')\n",
    "\n",
    "                                        window = rescale_data(window, min_mV, max_mV)\n",
    "                                        ##################\n",
    "                                        ## Quality type ##\n",
    "                                        ##################\n",
    "                                        # If we only retrieve the Excellent quality segments\n",
    "                                        if quality_type == 'excellent':\n",
    "                                            quality = nk.ecg_quality(window, sampling_rate=sample_rate, method=\"zhao2018\", approach=\"fuzzy\")\n",
    "                                            if quality == 'Excellent':\n",
    "                                                # Save the window to a file\n",
    "                                                np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_1.csv\"), window, delimiter=\",\")\n",
    "                                            else:\n",
    "                                                ignored_segments += 1\n",
    "                                        # If we retrieve all the segments\n",
    "                                        else:\n",
    "                                            np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_1.csv\"), window, delimiter=\",\")\n",
    "\n",
    "                                i += overlapping\n",
    "                                total_segments += 1\n",
    "                            else:\n",
    "                                i += 1\n",
    "                        else:\n",
    "                            i += 1\n",
    "                else:\n",
    "                    logging.info(f\"Processing NSRDB data for file: {file}\")\n",
    "                    # Split the cleaned signal into 10-second segments (window_length points at sample_rate Hz)\n",
    "                    segments = np.array_split(cleaned_ecg, len(cleaned_ecg) // window_length, axis=0)\n",
    "                    for i, segment in enumerate(segments):\n",
    "                        if not np.any(segment == 0):\n",
    "                            if len(segment) == window_length:\n",
    "                                #################\n",
    "                                ### Loop type ###\n",
    "                                #################\n",
    "                                total_segments += 1\n",
    "                                segment = nk.ecg_invert(segment, sampling_rate=record.fs)[0]\n",
    "                                segment = nk.signal_resample(segment, method=\"FFT\", sampling_rate=record.fs, desired_sampling_rate=sample_rate)\n",
    "                                if loop_type == 'manual':\n",
    "                                    segment = apply_bandpass_filter(segment, lowcut, highcut, sample_rate, order=5)\n",
    "                                    segment = wavelet_denoising(segment)\n",
    "                                elif loop_type == 'neurokit2':\n",
    "                                    segment = nk.ecg_clean(segment, sampling_rate=sample_rate, method='neurokit2')\n",
    "\n",
    "                                segment = rescale_data(segment, min_mV, max_mV)\n",
    "                                    \n",
    "                                ##################\n",
    "                                ## Quality type ##\n",
    "                                ##################\n",
    "                                if quality_type == 'excellent':\n",
    "                                    quality = nk.ecg_quality(segment, sampling_rate=sample_rate, method=\"zhao2018\", approach=\"fuzzy\")\n",
    "                                    if quality == 'Excellent':\n",
    "                                        np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_0.csv\"), segment, delimiter=\",\")\n",
    "                                    else:\n",
    "                                        ignored_segments += 1\n",
    "                                else:\n",
    "                                    np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_0.csv\"), segment, delimiter=\",\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                logging.error(f\"Exception occurred: {e}\")\n",
    "    \n",
    "    return total_segments, ignored_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SECONDS MODELS\n",
    "\n",
    "First load the data and labels from the CSV files, then prepare the data for the models and finally run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_run_models(seconds=10, results_subdir=None, actual_folder=None):\n",
    "    train_dir = os.path.join(preprocessing_dir, actual_folder, 'train')\n",
    "    val_dir = os.path.join(preprocessing_dir, actual_folder, 'val')\n",
    "    test_dir = os.path.join(preprocessing_dir, actual_folder, 'test')\n",
    "\n",
    "    train_files = os.listdir(train_dir)\n",
    "    val_files = os.listdir(val_dir)\n",
    "    test_files = os.listdir(test_dir)\n",
    "\n",
    "    train_data, train_labels = load_data_and_labels(train_files, train_dir)\n",
    "    val_data, val_labels = load_data_and_labels(val_files, val_dir)\n",
    "    test_data, test_labels = load_data_and_labels(test_files, test_dir)\n",
    "\n",
    "    frequency_used = sample_rate * seconds\n",
    "    train_data = np.array(train_data).reshape(-1, frequency_used, 1)\n",
    "    val_data = np.array(val_data).reshape(-1, frequency_used, 1)\n",
    "    test_data = np.array(test_data).reshape(-1, frequency_used, 1)\n",
    "    \n",
    "    train_labels = np.array(train_labels).astype('float32')\n",
    "    val_labels = np.array(val_labels).astype('float32')\n",
    "    test_labels = np.array(test_labels).astype('float32')\n",
    "\n",
    "    # Shuffle data\n",
    "    train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "    val_data, val_labels = shuffle(val_data, val_labels, random_state=42)\n",
    "    test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "\n",
    "    # Apply undersampling on train data\n",
    "    rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    train_data, train_labels = rus.fit_resample(train_data.reshape(train_data.shape[0], -1), train_labels)\n",
    "    train_data = train_data.reshape(-1, frequency_used, 1)\n",
    "\n",
    "    ###########\n",
    "    ### CNN ###\n",
    "    ###########\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "    model.add(Conv1D(64, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(128, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'CNN', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    ############\n",
    "    ### LSTM ###\n",
    "    ############\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "    model.add(LSTM(64)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu')) \n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'LSTM', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    # #####################\n",
    "    # ### RANDOM FOREST ###\n",
    "    # #####################\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    evaluate_model_RF(model, \"RandomForest\", train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "    \n",
    "    ##############\n",
    "    ### ResNet ###\n",
    "    ##############\n",
    "    model = ResNet1D(train_data.shape[1], train_data.shape[2])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'ResNet', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEATS\n",
    "\n",
    "BEATS PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ecg_data_beats(directory, segments_directory, subfolder, is_nsrdb=False, is_afdb=False, is_mitdb=False, patient_id=None, n_beats=10, limited_time=None, loop_type='manual', quality_type='excellent'):\n",
    "    total_segments = 0\n",
    "    ignored_segments = 0\n",
    "    if patient_id is not None:\n",
    "        files = [file for file in os.listdir(directory) if file.startswith(patient_id)]\n",
    "    else:\n",
    "        files = os.listdir(directory)\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith(\".dat\"):\n",
    "            logging.info(f\"Processing file: {file}\")\n",
    "\n",
    "            # Determine where to save the file based on the dataframe\n",
    "            current_set = ecg_df.loc[ecg_df['fileName'] == os.path.join(directory, file), 'set'].values[0]\n",
    "            save_dir = os.path.join(segments_directory, current_set)\n",
    "\n",
    "            record = wfdb.rdrecord(os.path.join(directory, file[:-4]))\n",
    "\n",
    "            if is_afdb :\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG2 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_nsrdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG1 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_mitdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('MLII')\n",
    "                except ValueError:\n",
    "                    print(f\"No MLII lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "\n",
    "            ecg_signals = record.p_signal[:, lead_index]\n",
    "            cleaned_ecg = ecg_signals\n",
    "            # If we work with limited time\n",
    "            if limited_time != None:\n",
    "                cleaned_ecg = ecg_signals[:record.fs * 60 * limited_time] # for now only work with limited time\n",
    "\n",
    "            try:\n",
    "              if is_afdb or is_mitdb:\n",
    "                annotation_extension = 'qrs' if is_afdb else 'atr'\n",
    "                qrs_annotations = wfdb.rdann(os.path.join(directory, file[:-4]), annotation_extension)\n",
    "                i = 0\n",
    "                while i < len(qrs_annotations.sample) - n_beats: # Ensure there are at least n_beats left\n",
    "                    if qrs_annotations.symbol[i] == 'N':\n",
    "                        next_symbols = qrs_annotations.symbol[i+1: i+n_beats]  \n",
    "                        if all(symbol == 'N' for symbol in next_symbols):\n",
    "                            start = qrs_annotations.sample[i]\n",
    "                            end = qrs_annotations.sample[i+n_beats]\n",
    "                            # Extract the window\n",
    "                            window = cleaned_ecg[start:end]\n",
    "                            if len(window) > 0:\n",
    "                                #################\n",
    "                                ### Loop type ###\n",
    "                                #################\n",
    "                                window = nk.ecg_invert(window, sampling_rate=record.fs)[0]\n",
    "                                window = nk.signal_resample(window, method=\"FFT\", sampling_rate=record.fs, desired_sampling_rate=sample_rate)\n",
    "                                if loop_type == 'manual':\n",
    "                                    window = apply_bandpass_filter(window, lowcut, highcut, sample_rate, order=5)\n",
    "                                    window = wavelet_denoising(window)\n",
    "                                elif loop_type == 'neurokit2':\n",
    "                                    window = nk.ecg_clean(window, sampling_rate=sample_rate, method='neurokit2')\n",
    "\n",
    "                                window = rescale_data(window, min_mV, max_mV)\n",
    "                                ##################\n",
    "                                ## Quality type ##\n",
    "                                ##################\n",
    "                                # If we only retrieve the Excellent quality segments\n",
    "                                if quality_type == 'excellent':\n",
    "                                    quality = nk.ecg_quality(window, sampling_rate=sample_rate, method=\"zhao2018\", approach=\"fuzzy\")\n",
    "                                    if quality == 'Excellent':\n",
    "                                        # Save the window to a file\n",
    "                                        np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_1.csv\"), window, delimiter=\",\")\n",
    "                                    else:\n",
    "                                        ignored_segments += 1\n",
    "                                # If we retrieve all the segments\n",
    "                                else:\n",
    "                                    np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_1.csv\"), window, delimiter=\",\")\n",
    "                            \n",
    "                            i += overlapping\n",
    "                            total_segments += 1\n",
    "                            continue\n",
    "                    i += 1\n",
    "              else:\n",
    "                qrs_annotations = wfdb.rdann(os.path.join(directory, file[:-4]), 'atr')\n",
    "                i = 0\n",
    "                while i < len(qrs_annotations.sample) - n_beats: # Ensure there are at least n_beats left\n",
    "                    next_symbols = qrs_annotations.symbol[i+1: i+n_beats] \n",
    "                    if all(symbol == 'N' for symbol in next_symbols):\n",
    "                        start = qrs_annotations.sample[i]\n",
    "                        end = qrs_annotations.sample[i+n_beats] \n",
    "                        # Extract the window\n",
    "                        window = cleaned_ecg[start:end]\n",
    "                        # Check if the window is empty\n",
    "                        if len(window) > 0:\n",
    "                            #################\n",
    "                            ### Loop type ###\n",
    "                            #################\n",
    "                            segment = nk.ecg_invert(window, sampling_rate=record.fs)[0]\n",
    "                            segment = nk.signal_resample(segment, method=\"FFT\", sampling_rate=record.fs, desired_sampling_rate=sample_rate)\n",
    "                            if loop_type == 'manual':\n",
    "                                segment = apply_bandpass_filter(segment, lowcut, highcut, sample_rate, order=5)\n",
    "                                segment = wavelet_denoising(segment)\n",
    "                            elif loop_type == 'neurokit2':\n",
    "                                segment = nk.ecg_clean(segment, sampling_rate=sample_rate, method='neurokit2')\n",
    "\n",
    "                            segment = rescale_data(segment, min_mV, max_mV)\n",
    "                                \n",
    "                            ##################\n",
    "                            ## Quality type ##\n",
    "                            ##################\n",
    "                            if quality_type == 'excellent':\n",
    "                                quality = nk.ecg_quality(segment, sampling_rate=sample_rate, method=\"zhao2018\", approach=\"fuzzy\")\n",
    "                                if quality == 'Excellent':\n",
    "                                    np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_0.csv\"), segment, delimiter=\",\")\n",
    "                                else:\n",
    "                                    ignored_segments += 1\n",
    "                            else:\n",
    "                                np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_0.csv\"), segment, delimiter=\",\")\n",
    "                        \n",
    "                        i += overlapping\n",
    "                        total_segments += 1\n",
    "                        continue\n",
    "                    i += 1\n",
    "            except Exception as e:\n",
    "              logging.error(f\"Exception occurred: {e}\")\n",
    "    return total_segments, ignored_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEATS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beats_run_models(results_subdir=None, actual_folder=None):\n",
    "    train_dir = os.path.join(preprocessing_dir, actual_folder, 'train')\n",
    "    val_dir = os.path.join(preprocessing_dir, actual_folder, 'val')\n",
    "    test_dir = os.path.join(preprocessing_dir, actual_folder, 'test')\n",
    "\n",
    "    train_files = os.listdir(train_dir)\n",
    "    val_files = os.listdir(val_dir)\n",
    "    test_files = os.listdir(test_dir)\n",
    "\n",
    "    train_data, train_labels = load_data_and_labels(train_files, train_dir)\n",
    "    val_data, val_labels = load_data_and_labels(val_files, val_dir)\n",
    "    test_data, test_labels = load_data_and_labels(test_files, test_dir)\n",
    "\n",
    "    # Determine the maximum length of the sequences\n",
    "    max_len = max(max(len(x) for x in train_data), max(len(x) for x in val_data), max(len(x) for x in test_data))\n",
    "    # Pad the sequences in each data array with -1\n",
    "    train_data = [np.pad(x, (0, max_len - len(x)), constant_values=-1) for x in train_data]\n",
    "    val_data = [np.pad(x, (0, max_len - len(x)), constant_values=-1) for x in val_data]\n",
    "    test_data = [np.pad(x, (0, max_len - len(x)), constant_values=-1) for x in test_data]\n",
    "    afdb_padding_percentage = calculate_padding_percentage(train_data)\n",
    "    mitdb_padding_percentage = calculate_padding_percentage(val_data)\n",
    "    nsrdb_padding_percentage = calculate_padding_percentage(test_data)\n",
    "    padding_percentages = {\n",
    "        \"TRAIN\": afdb_padding_percentage,\n",
    "        \"VAL\": mitdb_padding_percentage,\n",
    "        \"TEST\": nsrdb_padding_percentage\n",
    "    }\n",
    "    plot_padding_percentages(padding_percentages, results_subdir)\n",
    "\n",
    "    train_data = np.array(train_data).reshape(-1, max_len, 1)\n",
    "    val_data = np.array(val_data).reshape(-1, max_len, 1)\n",
    "    test_data = np.array(test_data).reshape(-1, max_len, 1)\n",
    "    \n",
    "    train_labels = np.array(train_labels).astype('float32')\n",
    "    val_labels = np.array(val_labels).astype('float32')\n",
    "    test_labels = np.array(test_labels).astype('float32')\n",
    "\n",
    "    # Shuffle data\n",
    "    train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "    val_data, val_labels = shuffle(val_data, val_labels, random_state=42)\n",
    "    test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "\n",
    "    # Apply undersampling on train data\n",
    "    rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    train_data, train_labels = rus.fit_resample(train_data.reshape(train_data.shape[0], -1), train_labels)\n",
    "    train_data = train_data.reshape(-1, max_len, 1)\n",
    "\n",
    "    ###########\n",
    "    ### CNN ###\n",
    "    ###########\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(max_len, 1)))\n",
    "    model.add(Conv1D(64, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(128, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'CNN', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    ############\n",
    "    ### LSTM ###\n",
    "    ############\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(max_len, 1)))\n",
    "    model.add(LSTM(64)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu')) \n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'LSTM', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    #####################\n",
    "    ### RANDOM FOREST ###\n",
    "    #####################\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    evaluate_model_RF(model, \"RandomForest\", train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "    \n",
    "    ##############\n",
    "    ### ResNet ###\n",
    "    ##############\n",
    "    model = ResNet1D(max_len,1)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'ResNet', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RR \n",
    "\n",
    "RR PREPROCESSING\n",
    "\n",
    "Based on the article by Duan et al. \n",
    "\n",
    "https://pubmed.ncbi.nlm.nih.gov/35925979/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ecg_data_RR(directory, segments_directory, subfolder, is_nsrdb=False, is_afdb=False, is_mitdb=False, patient_id=None, RRs=10, limited_time=None):\n",
    "    if patient_id is not None:\n",
    "        files = [file for file in os.listdir(directory) if file.startswith(patient_id)]\n",
    "    else:\n",
    "        files = os.listdir(directory)\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith(\".dat\"):\n",
    "            logging.info(f\"Processing file: {file}\")\n",
    "\n",
    "            # Determine where to save the file based on the dataframe\n",
    "            current_set = ecg_df.loc[ecg_df['fileName'] == os.path.join(directory, file), 'set'].values[0]\n",
    "            save_dir = os.path.join(segments_directory, current_set)\n",
    "\n",
    "            record = wfdb.rdrecord(os.path.join(directory, file[:-4]))\n",
    "\n",
    "            if is_afdb :\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG2 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_nsrdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG1 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_mitdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('MLII')\n",
    "                except ValueError:\n",
    "                    print(f\"No MLII lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "\n",
    "            ecg_signals = record.p_signal[:, lead_index]\n",
    "            cleaned_ecg = ecg_signals\n",
    "            # If we work with limited time\n",
    "            if limited_time != None:\n",
    "                cleaned_ecg = ecg_signals[:record.fs * 60 * limited_time] # for now only work with limited time\n",
    "\n",
    "            try:\n",
    "              if is_afdb or is_mitdb:\n",
    "                logging.info(f\"Processing AFDB or MITDB data for file: {file}\")\n",
    "                annotation_extension = 'qrs' if is_afdb else 'atr'\n",
    "                qrs_annotations = wfdb.rdann(os.path.join(directory, file[:-4]), annotation_extension)\n",
    "                i = 0\n",
    "                while i < len(qrs_annotations.sample) - RRs and qrs_annotations.sample[i] < len(cleaned_ecg):\n",
    "                    if qrs_annotations.symbol[i] == 'N':\n",
    "                        next_symbols = qrs_annotations.symbol[i+1: i+RRs]\n",
    "                        if all(symbol == 'N' for symbol in next_symbols):\n",
    "                            start = qrs_annotations.sample[i]\n",
    "                            end = qrs_annotations.sample[i+RRs]\n",
    "                            # Locate the indices of R-peaks that fall within the window\n",
    "                            rpeaks = [i for i in qrs_annotations.sample if start <= i < end]\n",
    "                            window = np.diff(rpeaks)\n",
    "                            # Resample to the frequency to get ms\n",
    "                            window = window / record.fs\n",
    "                            if len(window) == RRs-1:\n",
    "                                np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_1.csv\"), window, delimiter=\",\")\n",
    "                    \n",
    "                            i+=overlapping\n",
    "                        else:\n",
    "                            i += 1\n",
    "                    i += 1\n",
    "              else:\n",
    "                  logging.info(f\"Processing NSRDB data for file: {file}\")\n",
    "                  qrs_annotations = wfdb.rdann(os.path.join(directory, file[:-4]), 'atr')\n",
    "                  i = 0\n",
    "                  while i < len(qrs_annotations.sample) - RRs and qrs_annotations.sample[i] < len(cleaned_ecg):\n",
    "                    if qrs_annotations.symbol[i] == 'N':\n",
    "                        next_symbols = qrs_annotations.symbol[i+1: i+RRs]\n",
    "                        if all(symbol == 'N' for symbol in next_symbols):\n",
    "                          start = qrs_annotations.sample[i]\n",
    "                          end = qrs_annotations.sample[i+RRs]\n",
    "                          # Locate the indices of R-peaks that fall within the window\n",
    "                          rpeaks = [i for i in qrs_annotations.sample if start <= i < end]\n",
    "                          segment = np.diff(rpeaks)\n",
    "                          # Resample to the frequency to get ms\n",
    "                          segment = segment / record.fs\n",
    "                          if len(segment) == RRs-1:\n",
    "                              np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_0.csv\"), segment, delimiter=\",\")\n",
    "                          \n",
    "                          i += overlapping\n",
    "                        else:\n",
    "                            i += 1\n",
    "                    i += 1\n",
    "            except Exception as e:\n",
    "              print(e)\n",
    "              logging.error(f\"Exception occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RR MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR_run_models(results_subdir=None, actual_folder=None):\n",
    "    train_dir = os.path.join(preprocessing_dir, actual_folder, 'train')\n",
    "    val_dir = os.path.join(preprocessing_dir, actual_folder, 'val')\n",
    "    test_dir = os.path.join(preprocessing_dir, actual_folder, 'test')\n",
    "\n",
    "    train_files = os.listdir(train_dir)\n",
    "    val_files = os.listdir(val_dir)\n",
    "    test_files = os.listdir(test_dir)\n",
    "\n",
    "    train_data, train_labels = load_data_and_labels(train_files, train_dir)\n",
    "    val_data, val_labels = load_data_and_labels(val_files, val_dir)\n",
    "    test_data, test_labels = load_data_and_labels(test_files, test_dir)\n",
    "\n",
    "    train_data = np.array(train_data).reshape(-1, RRs-1, 1)\n",
    "    val_data = np.array(val_data).reshape(-1, RRs-1, 1)\n",
    "    test_data = np.array(test_data).reshape(-1, RRs-1, 1)\n",
    "    \n",
    "    train_labels = np.array(train_labels).astype('float32')\n",
    "    val_labels = np.array(val_labels).astype('float32')\n",
    "    test_labels = np.array(test_labels).astype('float32')\n",
    "\n",
    "    # Shuffle data\n",
    "    train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "    val_data, val_labels = shuffle(val_data, val_labels, random_state=42)\n",
    "    test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "\n",
    "    # Apply undersampling on train data\n",
    "    rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    train_data, train_labels = rus.fit_resample(train_data.reshape(train_data.shape[0], -1), train_labels)\n",
    "    train_data = train_data.reshape(-1, RRs-1, 1)\n",
    "\n",
    "    ###########\n",
    "    ### CNN ###\n",
    "    ###########\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(RRs-1,1)))\n",
    "    model.add(Conv1D(64, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(128, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'CNN', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    ############\n",
    "    ### LSTM ###\n",
    "    ############\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(RRs-1,1)))\n",
    "    model.add(LSTM(64)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu')) \n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'LSTM', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "\n",
    "    #####################\n",
    "    ### RANDOM FOREST ###\n",
    "    #####################\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    evaluate_model_RF(model, \"RandomForest\", train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "    \n",
    "    ##############\n",
    "    ### ResNet ###\n",
    "    ##############\n",
    "    model = ResNet1D(RRs-1,1) \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'ResNet', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEAKS\n",
    "\n",
    "PEAKS PREPROCESSING\n",
    "\n",
    "Based on the article of Castro et al. \n",
    "\n",
    "https://ijece.iaescore.com/index.php/IJECE/article/view/21303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ecg_data_pwave(directory, segments_directory, subfolder, is_nsrdb=False, is_afdb=False, is_mitdb=False, patient_id=None, RRs=10, limited_time=None):\n",
    "    if patient_id is not None:\n",
    "        files = [file for file in os.listdir(directory) if file.startswith(patient_id)]\n",
    "    else:\n",
    "        files = os.listdir(directory)\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith(\".dat\"):\n",
    "            logging.info(f\"Processing file: {file}\")\n",
    "\n",
    "            # Determine where to save the file based on the dataframe\n",
    "            current_set = ecg_df.loc[ecg_df['fileName'] == os.path.join(directory, file), 'set'].values[0]\n",
    "            save_dir = os.path.join(segments_directory, current_set)\n",
    "\n",
    "            record = wfdb.rdrecord(os.path.join(directory, file[:-4]))\n",
    "\n",
    "            if is_afdb :\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG2 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_nsrdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('ECG1')\n",
    "                except ValueError:\n",
    "                    print(f\"No ECG1 lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "            elif is_mitdb:\n",
    "                try:\n",
    "                    lead_index = record.sig_name.index('MLII')\n",
    "                except ValueError:\n",
    "                    print(f\"No MLII lead in {file}. Skipping this file.\")\n",
    "                    continue\n",
    "\n",
    "            ecg_signals = record.p_signal[:, lead_index]\n",
    "            cleaned_ecg = ecg_signals\n",
    "            # If we work with limited time\n",
    "            if limited_time != None:\n",
    "                cleaned_ecg = ecg_signals[:record.fs * 60 * limited_time]\n",
    "\n",
    "            try:\n",
    "              if is_afdb or is_mitdb:\n",
    "                  logging.info(f\"Processing AFDB or MITDB data for file: {file}\")\n",
    "                  annotation_extension = 'qrs' if is_afdb else 'atr'\n",
    "                  qrs_annotations = wfdb.rdann(os.path.join(directory, file[:-4]), annotation_extension)\n",
    "                  i = 0\n",
    "                  while i < len(qrs_annotations.sample) - RRs and qrs_annotations.sample[i] < len(cleaned_ecg):\n",
    "                      if qrs_annotations.symbol[i] == 'N':\n",
    "                          next_symbols = qrs_annotations.symbol[i+1: i+RRs]\n",
    "                          if all(symbol == 'N' for symbol in next_symbols):\n",
    "                              start = qrs_annotations.sample[i]\n",
    "                              end = qrs_annotations.sample[i+RRs]\n",
    "                              window = cleaned_ecg[start:end]\n",
    "                              processed_ecg, info = nk.ecg_process(window, sampling_rate=record.fs)\n",
    "\n",
    "                              all_peak_values = np.empty((RRs, 5))\n",
    "                              all_peak_values.fill(0)\n",
    "                              for peak_idx, peak_type in enumerate(['ECG_P_Peaks', 'ECG_Q_Peaks', 'ECG_R_Peaks', 'ECG_S_Peaks', 'ECG_T_Peaks']):\n",
    "                                  peak_indices = np.where(processed_ecg[peak_type] == 1)[0]\n",
    "                                  peak_values = processed_ecg['ECG_Clean'].iloc[peak_indices]\n",
    "                                  if len(peak_values) == 0:\n",
    "                                      all_peak_values[:, peak_idx] = 0\n",
    "                                  else:\n",
    "                                      num_peaks = min(len(peak_values), RRs)\n",
    "                                      all_peak_values[:num_peaks, peak_idx] = peak_values[:num_peaks].values\n",
    "                              all_peak_values = rescale_data_2D(all_peak_values, min_mV, max_mV)\n",
    "                              np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_1.csv\"), all_peak_values, delimiter=\",\")\n",
    "                              i+=overlapping\n",
    "                          else:\n",
    "                              i += 1\n",
    "                      else:\n",
    "                          i += 1\n",
    "              else:\n",
    "                  logging.info(f\"Processing NSRDB data for file: {file}\")\n",
    "                  qrs_annotations = wfdb.rdann(os.path.join(directory, file[:-4]), 'atr')\n",
    "                  i = 0\n",
    "                  while i < len(qrs_annotations.sample) - RRs and qrs_annotations.sample[i] < len(cleaned_ecg):  \n",
    "                      if qrs_annotations.symbol[i] == 'N':\n",
    "                          next_symbols = qrs_annotations.symbol[i+1: i+RRs]  \n",
    "                          if all(symbol == 'N' for symbol in next_symbols):\n",
    "                              start = qrs_annotations.sample[i]\n",
    "                              end = qrs_annotations.sample[i+RRs]\n",
    "                              window = cleaned_ecg[start:end]\n",
    "                              processed_ecg, info = nk.ecg_process(window, sampling_rate=record.fs)\n",
    "\n",
    "                              all_peak_values = np.empty((RRs, 5))\n",
    "                              all_peak_values.fill(0) \n",
    "                              for peak_idx, peak_type in enumerate(['ECG_P_Peaks', 'ECG_Q_Peaks', 'ECG_R_Peaks', 'ECG_S_Peaks', 'ECG_T_Peaks']):\n",
    "                                  peak_indices = np.where(processed_ecg[peak_type] == 1)[0]\n",
    "                                  peak_values = processed_ecg['ECG_Clean'].iloc[peak_indices]\n",
    "                                  if len(peak_values) == 0:\n",
    "                                      all_peak_values[:, peak_idx] = 0\n",
    "                                  else:\n",
    "                                      num_peaks = min(len(peak_values), RRs)\n",
    "                                      all_peak_values[:num_peaks, peak_idx] = peak_values[:num_peaks].values\n",
    "                              all_peak_values = rescale_data_2D(all_peak_values, min_mV, max_mV)\n",
    "                              np.savetxt(os.path.join(save_dir, f\"{file[:-4]}_segment_{i}_0.csv\"), all_peak_values, delimiter=\",\")\n",
    "                              i += overlapping\n",
    "                          else:\n",
    "                              i += 1\n",
    "                      else:\n",
    "                          i += 1\n",
    " \n",
    "            except Exception as e:\n",
    "              print(e)\n",
    "              logging.error(f\"Exception occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEAKS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pwave_run_models(results_subdir=None, actual_folder=None):\n",
    "    train_dir = os.path.join(preprocessing_dir, actual_folder, 'train')\n",
    "    val_dir = os.path.join(preprocessing_dir, actual_folder, 'val')\n",
    "    test_dir = os.path.join(preprocessing_dir, actual_folder, 'test')\n",
    "\n",
    "    train_files = os.listdir(train_dir)\n",
    "    val_files = os.listdir(val_dir)\n",
    "    test_files = os.listdir(test_dir)\n",
    "\n",
    "    train_data, train_labels = load_data_and_labels(train_files, train_dir)\n",
    "    val_data, val_labels = load_data_and_labels(val_files, val_dir)\n",
    "    test_data, test_labels = load_data_and_labels(test_files, test_dir)\n",
    "\n",
    "    train_data = np.array(train_data).reshape(len(train_files), RRs, 5)\n",
    "    val_data = np.array(val_data).reshape(len(val_files), RRs, 5)\n",
    "    test_data = np.array(test_data).reshape(len(test_files), RRs, 5)\n",
    "    \n",
    "    train_labels = np.array(train_labels).astype('float32')\n",
    "    val_labels = np.array(val_labels).astype('float32')\n",
    "    test_labels = np.array(test_labels).astype('float32')\n",
    "\n",
    "    # Shuffle data\n",
    "    train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "    val_data, val_labels = shuffle(val_data, val_labels, random_state=42)\n",
    "    test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "\n",
    "    # Apply undersampling on train data\n",
    "    rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "    train_data, train_labels = rus.fit_resample(train_data.reshape(train_data.shape[0], -1), train_labels)\n",
    "    train_data = train_data.reshape(-1, RRs, 5)\n",
    "\n",
    "    # Replace nan values with zero in the data\n",
    "    train_data = np.nan_to_num(train_data)\n",
    "    val_data = np.nan_to_num(val_data)\n",
    "    test_data = np.nan_to_num(test_data)\n",
    "\n",
    "    ###########\n",
    "    ### CNN ###\n",
    "    ###########\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "    model.add(Conv1D(64, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(128, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'CNN', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    ############\n",
    "    ### LSTM ###\n",
    "    ############\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-1., input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "    model.add(LSTM(64)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu')) \n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'LSTM', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    #####################\n",
    "    ### RANDOM FOREST ###\n",
    "    #####################\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    evaluate_model_RF(model, \"RandomForest\", train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)\n",
    "\n",
    "    ##############\n",
    "    ### ResNet ###\n",
    "    ##############\n",
    "    model = ResNet1D(train_data.shape[1], train_data.shape[2])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    evaluate_model(model, 'ResNet', train_data, train_labels, val_data, val_labels, test_data, test_labels, results_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING LOOP\n",
    "Loop on defined parameters to generate all preprocessed data and compute first results.\n",
    "\n",
    "Loop parameters can be changed in the first cell of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(preprocessing_dir):\n",
    "   shutil.rmtree(preprocessing_dir)\n",
    "os.makedirs(preprocessing_dir)\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "   shutil.rmtree(results_dir)\n",
    "os.makedirs(results_dir)\n",
    "\n",
    "segments_dir_dict = {\n",
    "    'seconds': sec_segments_dir,\n",
    "    'beats': beats_segments_dir,\n",
    "    'RR': RR_segments_dir,\n",
    "    'pwave': pwave_segments_dir\n",
    "}\n",
    "\n",
    "#####################\n",
    "for limited_time in limited_time_list:\n",
    "    for loop_type in loop_list:\n",
    "        for quality_type in quality_list:\n",
    "            for analyse_type in analyse_list:\n",
    "              print('Looping on :', analyse_type, loop_type, quality_type, limited_time)\n",
    "              results_subdir = os.path.join(results_dir, f\"{analyse_type}_{loop_type}_{quality_type}_{limited_time}\")\n",
    "              os.mkdir(results_subdir)\n",
    "              subfolder = f\"{analyse_type}_{loop_type}_{quality_type}\"\n",
    "              segments_dir = segments_dir_dict[analyse_type]\n",
    "              preprocessing_subdir = os.path.join(segments_dir, f\"{analyse_type}_{loop_type}_{quality_type}_{limited_time}\")\n",
    "              os.makedirs(preprocessing_subdir, exist_ok=True)\n",
    "              train_dir = os.path.join(preprocessing_subdir, \"train\")\n",
    "              val_dir = os.path.join(preprocessing_subdir, \"val\")\n",
    "              test_dir = os.path.join(preprocessing_subdir, \"test\")\n",
    "              os.makedirs(train_dir, exist_ok=True)\n",
    "              os.makedirs(val_dir, exist_ok=True)\n",
    "              os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "              \n",
    "              #####################\n",
    "              ### SECONDS model ###\n",
    "              #####################\n",
    "              if analyse_type == 'seconds':\n",
    "                  afdb_total_segments, afdb_ignored_segments = process_ecg_data_seconds(afdb_dir, preprocessing_subdir, \"afdb\", is_afdb=True, seconds=seconds, limited_time=limited_time, loop_type=loop_type, quality_type=quality_type)\n",
    "                  mitdb_total_segments, mitdb_ignored_segments = process_ecg_data_seconds(mitdb_dir, preprocessing_subdir, \"mitdb\", is_mitdb=True, seconds=seconds, limited_time=limited_time, loop_type=loop_type, quality_type=quality_type)\n",
    "                  nsrdb_total_segments, nsrdb_ignored_segments = process_ecg_data_seconds(nsrdb_dir, preprocessing_subdir, \"nsrdb\", is_nsrdb=True, seconds=seconds, limited_time=limited_time, loop_type=loop_type, quality_type=quality_type)\n",
    "                  if quality_type == 'excellent':\n",
    "                    plot_segment_percentages(results_subdir,{'afdb': afdb_ignored_segments, 'mitdb': mitdb_ignored_segments, 'nsrdb': nsrdb_ignored_segments}, {'afdb': afdb_total_segments, 'mitdb': mitdb_total_segments, 'nsrdb': nsrdb_total_segments})\n",
    "                  data_analysis(preprocessing_subdir, results_subdir)\n",
    "\n",
    "              ####################\n",
    "              ### BEATS model ####\n",
    "              ####################\n",
    "              if analyse_type == 'beats':\n",
    "                  afdb_total_segments, afdb_ignored_segments = process_ecg_data_beats(afdb_dir, preprocessing_subdir, \"afdb\", is_afdb=True, n_beats=n_beats, limited_time=limited_time, loop_type=loop_type, quality_type=quality_type)\n",
    "                  mitdb_total_segments, mitdb_ignored_segments = process_ecg_data_beats(mitdb_dir, preprocessing_subdir, \"mitdb\", is_mitdb=True, n_beats=n_beats, limited_time=limited_time, loop_type=loop_type, quality_type=quality_type)\n",
    "                  nsrdb_total_segments, nsrdb_ignored_segments = process_ecg_data_beats(nsrdb_dir, preprocessing_subdir, \"nsrdb\", is_nsrdb=True, n_beats=n_beats, limited_time=limited_time, loop_type=loop_type, quality_type=quality_type)\n",
    "                  if quality_type == 'excellent':\n",
    "                    plot_segment_percentages(results_subdir,{'afdb': afdb_ignored_segments, 'mitdb': mitdb_ignored_segments, 'nsrdb': nsrdb_ignored_segments}, {'afdb': afdb_total_segments, 'mitdb': mitdb_total_segments, 'nsrdb': nsrdb_total_segments})\n",
    "                  data_analysis(preprocessing_subdir, results_subdir)\n",
    "\n",
    "              ################\n",
    "              ### RR model ###\n",
    "              ################\n",
    "              if analyse_type == 'RR' and loop_type == 'manual' and quality_type == 'excellent':\n",
    "                  process_ecg_data_RR(afdb_dir, preprocessing_subdir, \"afdb\", is_afdb=True, RRs=RRs, limited_time=limited_time)\n",
    "                  process_ecg_data_RR(mitdb_dir, preprocessing_subdir, \"mitdb\", is_mitdb=True, RRs=RRs, limited_time=limited_time)\n",
    "                  process_ecg_data_RR(nsrdb_dir, preprocessing_subdir, \"nsrdb\", is_nsrdb=True, RRs=RRs, limited_time=limited_time)\n",
    "                  data_analysis(preprocessing_subdir, results_subdir)               \n",
    "\n",
    "              #############\n",
    "              ### PWAVE ###\n",
    "              #############\n",
    "              if analyse_type == 'pwave' and loop_type == 'manual' and quality_type == 'excellent':\n",
    "                process_ecg_data_pwave(afdb_dir, preprocessing_subdir, \"afdb\", is_afdb=True, RRs=RRs, limited_time=limited_time)\n",
    "                process_ecg_data_pwave(mitdb_dir, preprocessing_subdir, \"mitdb\", is_mitdb=True, RRs=RRs, limited_time=limited_time)\n",
    "                process_ecg_data_pwave(nsrdb_dir, preprocessing_subdir, \"nsrdb\", is_nsrdb=True, RRs=RRs, limited_time=limited_time)\n",
    "                data_analysis(preprocessing_subdir, results_subdir)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS LOOP\n",
    "\n",
    "First get the data from the preprocessing folder.\n",
    "\n",
    "Then run the models on the data, collect the results and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop on all the preprocessing folders and run the models on them\n",
    "# Loop parameters are defined in the first cell of the notebook\n",
    "for root, dirs, files in os.walk(preprocessing_dir):\n",
    "    for folder in dirs:\n",
    "        if len(folder.split('_')) == 4:\n",
    "            analyse_type, loop_type, quality_type, limited_time = folder.split('_')\n",
    "            if analyse_type in analyse_list and loop_type in loop_list and quality_type in quality_list and int(limited_time) in limited_time_list:\n",
    "                print('Looping on :', analyse_type, loop_type, quality_type, limited_time)\n",
    "                results_subdir = os.path.join(results_dir, folder)\n",
    "                os.makedirs(results_subdir, exist_ok=True)\n",
    "                if analyse_type == 'seconds':\n",
    "                    sec_run_models(seconds=seconds, results_subdir=results_subdir, actual_folder=folder)\n",
    "                elif analyse_type == 'beats':\n",
    "                    beats_run_models(results_subdir=results_subdir, actual_folder=folder)\n",
    "                elif analyse_type == 'RR' and loop_type == 'manual' and quality_type == 'excellent':\n",
    "                    RR_run_models(results_subdir=results_subdir, actual_folder=folder)\n",
    "                elif analyse_type == 'pwave' and loop_type == 'manual' and quality_type == 'excellent':\n",
    "                    pwave_run_models(results_subdir=results_subdir, actual_folder=folder)\n",
    "                \n",
    "collect_results(results_dir)\n",
    "df = pd.read_csv(os.path.join(results_dir, 'complete_results.csv'))\n",
    "plot_results(df, results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
